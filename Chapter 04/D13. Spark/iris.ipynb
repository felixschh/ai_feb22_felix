{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130da3b8",
   "metadata": {},
   "source": [
    "## SQL and DataFrames\n",
    "\n",
    "There are two approaches to Spark, the DataFrame approach and the RDD approach. We are going to learn the SQL approach since it is works in the way spark intends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a6891b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7664a",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "Spark needs to use a session in order to process data in a parallel way.\n",
    "\n",
    "A session can be built in many different ways, what is going to be a difference maker for most local machines is that we need to specify to spark to either get it or create it.\n",
    "\n",
    "We will use this session to define our Spark DataFrames.\n",
    "\n",
    "When Creating DataFrames we can let spark infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a47c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c588c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/21 20:12:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"iris_clf\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5b45e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Setosa|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/iris.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a85e8",
   "metadata": {},
   "source": [
    "Or we can create our own schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6724c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"sepal_length\", DoubleType()),\n",
    "    StructField(\"sepal_width\", DoubleType()),\n",
    "    StructField(\"petal_length\", DoubleType()),\n",
    "    StructField(\"petal_width\", DoubleType()),\n",
    "    StructField(\"type\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083f8067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Setosa|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv('data/iris.csv', header=True, schema=schema)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e2d28",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Spark doesn`t need the X and Y separated in the standard format.\n",
    "\n",
    "As you will see throught the notebook spark will directly operate on the DataFrame after specifying an input and aoutput column.\n",
    "\n",
    "By default Spark takes a column called deatures as the input in all classifiers and the Y column is called labels\n",
    "\n",
    "We can create the  feature column by using a vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7601d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4123838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Setosa|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2|Setosa|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2|Setosa|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2|Setosa|[5.0,3.6,1.4,0.2]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_col = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "vectorizer = VectorAssembler(inputCols = input_col, outputCol='features')\n",
    "\n",
    "df = vectorizer.transform(df)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e9df0",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "We can use a string indexes in the same way as the vector assembler to ordinally encode our types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f6ed76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace8851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|         features|indexed_type|\n",
      "+------------+-----------+------------+-----------+------+-----------------+------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Setosa|[5.1,3.5,1.4,0.2]|         0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|Setosa|[4.9,3.0,1.4,0.2]|         0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|Setosa|[4.7,3.2,1.3,0.2]|         0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|[4.6,3.1,1.5,0.2]|         0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|Setosa|[5.0,3.6,1.4,0.2]|         0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='type', outputCol='indexed_type')\n",
    "df = indexer.fit(df).transform(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653bbee7",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "Spark Dataframes come pre-equipped with a random split function that will give you as many portions as specified.\n",
    "\n",
    "The proportions for each portion are passed in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "036b95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+-----------------+------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      type|         features|indexed_type|\n",
      "+------------+-----------+------------+-----------+----------+-----------------+------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|    Setosa|[4.3,3.0,1.1,0.1]|         0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2|    Setosa|[4.4,2.9,1.4,0.2]|         0.0|\n",
      "|         4.4|        3.0|         1.3|        0.2|    Setosa|[4.4,3.0,1.3,0.2]|         0.0|\n",
      "|         4.4|        3.2|         1.3|        0.2|    Setosa|[4.4,3.2,1.3,0.2]|         0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|    Setosa|[4.6,3.2,1.4,0.2]|         0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|    Setosa|[4.6,3.4,1.4,0.3]|         0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|    Setosa|[4.6,3.6,1.0,0.2]|         0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|    Setosa|[4.7,3.2,1.3,0.2]|         0.0|\n",
      "|         4.7|        3.2|         1.6|        0.2|    Setosa|[4.7,3.2,1.6,0.2]|         0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|    Setosa|[4.8,3.0,1.4,0.1]|         0.0|\n",
      "|         4.8|        3.0|         1.4|        0.3|    Setosa|[4.8,3.0,1.4,0.3]|         0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|    Setosa|[4.9,3.0,1.4,0.2]|         0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    Setosa|[4.9,3.1,1.5,0.1]|         0.0|\n",
      "|         4.9|        3.1|         1.5|        0.2|    Setosa|[4.9,3.1,1.5,0.2]|         0.0|\n",
      "|         4.9|        3.6|         1.4|        0.1|    Setosa|[4.9,3.6,1.4,0.1]|         0.0|\n",
      "|         5.0|        2.0|         3.5|        1.0|Versicolor|[5.0,2.0,3.5,1.0]|         1.0|\n",
      "|         5.0|        2.3|         3.3|        1.0|Versicolor|[5.0,2.3,3.3,1.0]|         1.0|\n",
      "|         5.0|        3.0|         1.6|        0.2|    Setosa|[5.0,3.0,1.6,0.2]|         0.0|\n",
      "|         5.0|        3.2|         1.2|        0.2|    Setosa|[5.0,3.2,1.2,0.2]|         0.0|\n",
      "|         5.0|        3.3|         1.4|        0.2|    Setosa|[5.0,3.3,1.4,0.2]|         0.0|\n",
      "+------------+-----------+------------+-----------+----------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df.randomSplit([0.7, 0.3], seed=1)\n",
    "\n",
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c34d89",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "Many Spark classifiers unfortunatelly do not handle good multylabel classification so be very carefull with which you choose.\n",
    "\n",
    "they can all be found here: https://spark.apache.org/docs/latest/ml-classification-regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0fc58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cca848",
   "metadata": {},
   "source": [
    "## Specifying input and target\n",
    "\n",
    "As I said the default names are features an label, but we can also specify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300b8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(featuresCol='features', labelCol='indexed_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023be53",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "It is done in the same way as SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570eea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = rf_clf.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3b7aa",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "    \n",
    "This part is a little different. Spark will not output a prediction vector, it will direclty add a column to the DataFrame.\n",
    "\n",
    "To predict we call the method 'transform' from the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f12cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+------------+--------------+---------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|  type|         features|indexed_type| rawPrediction|    probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+------------+--------------+---------------+----------+\n",
      "|         4.5|        2.3|         1.3|        0.3|Setosa|[4.5,2.3,1.3,0.3]|         0.0|[20.0,0.0,0.0]|  [1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|Setosa|[4.6,3.1,1.5,0.2]|         0.0|[20.0,0.0,0.0]|  [1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|Setosa|[4.8,3.1,1.6,0.2]|         0.0|[20.0,0.0,0.0]|  [1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.4|         1.6|        0.2|Setosa|[4.8,3.4,1.6,0.2]|         0.0|[20.0,0.0,0.0]|  [1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.4|         1.9|        0.2|Setosa|[4.8,3.4,1.9,0.2]|         0.0|[15.0,5.0,0.0]|[0.75,0.25,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+------------+--------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = rf_clf.transform(df_test)\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a7c217",
   "metadata": {},
   "source": [
    "## Selecting\n",
    "\n",
    "This structures are built to be parallelized in the CPU so we cannot access them in a standard fashion.\n",
    "\n",
    "To get a subset of columns we need to use select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a0de855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------+----------+\n",
      "|      type|    probability|indexed_type|prediction|\n",
      "+----------+---------------+------------+----------+\n",
      "|    Setosa|  [1.0,0.0,0.0]|         0.0|       0.0|\n",
      "|    Setosa|  [1.0,0.0,0.0]|         0.0|       0.0|\n",
      "|    Setosa|  [1.0,0.0,0.0]|         0.0|       0.0|\n",
      "|    Setosa|  [1.0,0.0,0.0]|         0.0|       0.0|\n",
      "|    Setosa|[0.75,0.25,0.0]|         0.0|       0.0|\n",
      "|Versicolor|  [0.0,1.0,0.0]|         1.0|       1.0|\n",
      "| Virginica|[0.0,0.85,0.15]|         2.0|       1.0|\n",
      "|    Setosa|  [1.0,0.0,0.0]|         0.0|       0.0|\n",
      "|Versicolor|  [0.0,1.0,0.0]|         1.0|       1.0|\n",
      "|    Setosa|[0.95,0.05,0.0]|         0.0|       0.0|\n",
      "+----------+---------------+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select('type','probability','indexed_type','prediction').show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "\n",
    "In a very familiar format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f286fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MulticlassClassificationEvaluator(labelCol='indexed_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55054ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accurcy of the predictions is : 92.53373313343329 %\n"
     ]
    }
   ],
   "source": [
    "acc = criterion.evaluate(df_test)\n",
    "print(f'The Accurcy of the predictions is : {acc*100} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('strive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3606f1f72cab31e12ded3fd4dc568aeec6faa77d43eaca4ad210e84657d2ac3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
