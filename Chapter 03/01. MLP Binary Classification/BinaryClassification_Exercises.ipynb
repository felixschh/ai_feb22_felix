{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hazardous-weather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:47:37.551793Z",
     "start_time": "2021-05-26T07:47:35.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "clean-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:50:32.482086Z",
     "start_time": "2021-05-26T07:50:32.456893Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a neural network class inheriting from the nn.Module\n",
    "# Call it NeuralNetwork and make, and use \"pass\" in the constructor\n",
    "# so that it doesn't give an error\n",
    "# Instantiate one instance of it in variable net\n",
    "\n",
    "df = pd.read_csv('data.csv', header=None)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(2, 16)\n",
    "        self.hidden1 = nn.Linear(16, 8)\n",
    "        self.hidden2 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_layer = self.input_layer(x)\n",
    "        act1 = self.sigmoid(first_layer)\n",
    "        second_layer = self.hidden1(act1)\n",
    "        act2 = self.sigmoid(second_layer)\n",
    "        thrid_layer = self.hidden2(act2)\n",
    "        act3 = self.sigmoid(thrid_layer)\n",
    "        out_layer = self.output(act3)\n",
    "        prediction = self.sigmoid(out_layer)\n",
    "        return prediction\n",
    "\n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "demographic-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:51:28.420569Z",
     "start_time": "2021-05-26T07:51:28.412916Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(net, NeuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "curious-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:11.203531Z",
     "start_time": "2021-05-26T07:56:11.199729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim and num_hidden, respectively the dimension of \n",
    "# the input and the number of hidden neurons\n",
    "# use pass again\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    pass\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super(NeuralNetwork).__init__()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bigger-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:27.491588Z",
     "start_time": "2021-05-26T08:04:27.484159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim, num_hidden1 and num_hidden2, respectively the dimension of \n",
    "# the input and the number of hidden neurons for the first fully connected\n",
    "# layer and the second. Define the attributes in the constructor\n",
    "# that consists of the layers, call them fc1, fc2 and fc3 and a sigmoid.\n",
    "# use pass again. Be careful to put the dimensions in the right places!\n",
    "# Since we will do a binary classification problem, fc3 will have 1 neuron\n",
    "# as output\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(2, 16)\n",
    "        self.hidden1 = nn.Linear(16, 8)\n",
    "        self.hidden2 = nn.Linear(8, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_layer = self.input_layer(x)\n",
    "        act1 = self.sigmoid(first_layer)\n",
    "        second_layer = self.hidden1(act1)\n",
    "        act2 = self.sigmoid(second_layer)\n",
    "        thrid_layer = self.hidden2(act2)\n",
    "        act3 = self.sigmoid(thrid_layer)\n",
    "        out_layer = self.output(act3)\n",
    "        prediction = self.sigmoid(out_layer)\n",
    "        return prediction\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "smart-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass to make a reasonable use of the attributes\n",
    "# you defined before. Follow the same reasoning we used in class\n",
    "\n",
    "net = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "933260ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fd198f50350>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "latest-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training a model, use the following optimizer and loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = T.optim.Adam(net.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "lesser-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss: 0.6988752484321594 \n",
      "Epoch: 2 | loss: 0.6986203789710999 \n",
      "Epoch: 3 | loss: 0.698371410369873 \n",
      "Epoch: 4 | loss: 0.6981285810470581 \n",
      "Epoch: 5 | loss: 0.6978920698165894 \n",
      "Epoch: 6 | loss: 0.697661817073822 \n",
      "Epoch: 7 | loss: 0.6974380612373352 \n",
      "Epoch: 8 | loss: 0.6972208619117737 \n",
      "Epoch: 9 | loss: 0.6970100402832031 \n",
      "Epoch: 10 | loss: 0.6968059539794922 \n",
      "Epoch: 11 | loss: 0.6966084241867065 \n",
      "Epoch: 12 | loss: 0.6964175701141357 \n",
      "Epoch: 13 | loss: 0.6962333917617798 \n",
      "Epoch: 14 | loss: 0.6960560083389282 \n",
      "Epoch: 15 | loss: 0.6958850622177124 \n",
      "Epoch: 16 | loss: 0.695720911026001 \n",
      "Epoch: 17 | loss: 0.6955631971359253 \n",
      "Epoch: 18 | loss: 0.6954121589660645 \n",
      "Epoch: 19 | loss: 0.6952673196792603 \n",
      "Epoch: 20 | loss: 0.6951290369033813 \n",
      "Epoch: 21 | loss: 0.6949969530105591 \n",
      "Epoch: 22 | loss: 0.6948711276054382 \n",
      "Epoch: 23 | loss: 0.6947513818740845 \n",
      "Epoch: 24 | loss: 0.6946374773979187 \n",
      "Epoch: 25 | loss: 0.6945293545722961 \n",
      "Epoch: 26 | loss: 0.694426953792572 \n",
      "Epoch: 27 | loss: 0.6943300366401672 \n",
      "Epoch: 28 | loss: 0.6942384243011475 \n",
      "Epoch: 29 | loss: 0.6941520571708679 \n",
      "Epoch: 30 | loss: 0.6940707564353943 \n",
      "Epoch: 31 | loss: 0.6939942240715027 \n",
      "Epoch: 32 | loss: 0.6939224004745483 \n",
      "Epoch: 33 | loss: 0.6938550472259521 \n",
      "Epoch: 34 | loss: 0.6937920451164246 \n",
      "Epoch: 35 | loss: 0.6937330365180969 \n",
      "Epoch: 36 | loss: 0.6936782002449036 \n",
      "Epoch: 37 | loss: 0.6936269998550415 \n",
      "Epoch: 38 | loss: 0.6935794353485107 \n",
      "Epoch: 39 | loss: 0.6935351490974426 \n",
      "Epoch: 40 | loss: 0.6934940814971924 \n",
      "Epoch: 41 | loss: 0.6934561133384705 \n",
      "Epoch: 42 | loss: 0.693420946598053 \n",
      "Epoch: 43 | loss: 0.6933885216712952 \n",
      "Epoch: 44 | loss: 0.6933585405349731 \n",
      "Epoch: 45 | loss: 0.6933309435844421 \n",
      "Epoch: 46 | loss: 0.693305492401123 \n",
      "Epoch: 47 | loss: 0.6932820081710815 \n",
      "Epoch: 48 | loss: 0.6932604312896729 \n",
      "Epoch: 49 | loss: 0.6932405233383179 \n",
      "Epoch: 50 | loss: 0.6932222247123718 \n",
      "Epoch: 51 | loss: 0.6932052373886108 \n",
      "Epoch: 52 | loss: 0.693189799785614 \n",
      "Epoch: 53 | loss: 0.6931753754615784 \n",
      "Epoch: 54 | loss: 0.6931620240211487 \n",
      "Epoch: 55 | loss: 0.693149745464325 \n",
      "Epoch: 56 | loss: 0.6931381225585938 \n",
      "Epoch: 57 | loss: 0.6931275129318237 \n",
      "Epoch: 58 | loss: 0.6931173801422119 \n",
      "Epoch: 59 | loss: 0.6931079030036926 \n",
      "Epoch: 60 | loss: 0.6930990815162659 \n",
      "Epoch: 61 | loss: 0.6930904984474182 \n",
      "Epoch: 62 | loss: 0.6930824518203735 \n",
      "Epoch: 63 | loss: 0.693074643611908 \n",
      "Epoch: 64 | loss: 0.6930670738220215 \n",
      "Epoch: 65 | loss: 0.6930598616600037 \n",
      "Epoch: 66 | loss: 0.6930528283119202 \n",
      "Epoch: 67 | loss: 0.6930457949638367 \n",
      "Epoch: 68 | loss: 0.6930390000343323 \n",
      "Epoch: 69 | loss: 0.6930322051048279 \n",
      "Epoch: 70 | loss: 0.6930255889892578 \n",
      "Epoch: 71 | loss: 0.6930189728736877 \n",
      "Epoch: 72 | loss: 0.6930122971534729 \n",
      "Epoch: 73 | loss: 0.6930056810379028 \n",
      "Epoch: 74 | loss: 0.6929990649223328 \n",
      "Epoch: 75 | loss: 0.6929923295974731 \n",
      "Epoch: 76 | loss: 0.6929855942726135 \n",
      "Epoch: 77 | loss: 0.6929787993431091 \n",
      "Epoch: 78 | loss: 0.6929720044136047 \n",
      "Epoch: 79 | loss: 0.6929651498794556 \n",
      "Epoch: 80 | loss: 0.6929581165313721 \n",
      "Epoch: 81 | loss: 0.6929511427879333 \n",
      "Epoch: 82 | loss: 0.6929440498352051 \n",
      "Epoch: 83 | loss: 0.6929368376731873 \n",
      "Epoch: 84 | loss: 0.6929296255111694 \n",
      "Epoch: 85 | loss: 0.6929222345352173 \n",
      "Epoch: 86 | loss: 0.6929149031639099 \n",
      "Epoch: 87 | loss: 0.6929073333740234 \n",
      "Epoch: 88 | loss: 0.6928997039794922 \n",
      "Epoch: 89 | loss: 0.6928920745849609 \n",
      "Epoch: 90 | loss: 0.6928842663764954 \n",
      "Epoch: 91 | loss: 0.6928764581680298 \n",
      "Epoch: 92 | loss: 0.6928684115409851 \n",
      "Epoch: 93 | loss: 0.6928604245185852 \n",
      "Epoch: 94 | loss: 0.692852258682251 \n",
      "Epoch: 95 | loss: 0.6928438544273376 \n",
      "Epoch: 96 | loss: 0.6928354501724243 \n",
      "Epoch: 97 | loss: 0.6928269863128662 \n",
      "Epoch: 98 | loss: 0.692818284034729 \n",
      "Epoch: 99 | loss: 0.6928095817565918 \n",
      "Epoch: 100 | loss: 0.6928007006645203 \n",
      "Epoch: 101 | loss: 0.6927915811538696 \n",
      "Epoch: 102 | loss: 0.692782461643219 \n",
      "Epoch: 103 | loss: 0.6927732229232788 \n",
      "Epoch: 104 | loss: 0.6927636861801147 \n",
      "Epoch: 105 | loss: 0.6927541494369507 \n",
      "Epoch: 106 | loss: 0.6927443742752075 \n",
      "Epoch: 107 | loss: 0.6927344799041748 \n",
      "Epoch: 108 | loss: 0.6927242279052734 \n",
      "Epoch: 109 | loss: 0.6927139163017273 \n",
      "Epoch: 110 | loss: 0.6927034258842468 \n",
      "Epoch: 111 | loss: 0.6926926970481873 \n",
      "Epoch: 112 | loss: 0.6926819086074829 \n",
      "Epoch: 113 | loss: 0.6926707625389099 \n",
      "Epoch: 114 | loss: 0.6926594376564026 \n",
      "Epoch: 115 | loss: 0.6926478743553162 \n",
      "Epoch: 116 | loss: 0.6926361918449402 \n",
      "Epoch: 117 | loss: 0.6926241517066956 \n",
      "Epoch: 118 | loss: 0.6926119327545166 \n",
      "Epoch: 119 | loss: 0.692599356174469 \n",
      "Epoch: 120 | loss: 0.6925866007804871 \n",
      "Epoch: 121 | loss: 0.6925735473632812 \n",
      "Epoch: 122 | loss: 0.6925601959228516 \n",
      "Epoch: 123 | loss: 0.6925466060638428 \n",
      "Epoch: 124 | loss: 0.6925326585769653 \n",
      "Epoch: 125 | loss: 0.6925184726715088 \n",
      "Epoch: 126 | loss: 0.6925038695335388 \n",
      "Epoch: 127 | loss: 0.6924890875816345 \n",
      "Epoch: 128 | loss: 0.6924738883972168 \n",
      "Epoch: 129 | loss: 0.6924582719802856 \n",
      "Epoch: 130 | loss: 0.6924424171447754 \n",
      "Epoch: 131 | loss: 0.6924260854721069 \n",
      "Epoch: 132 | loss: 0.6924094557762146 \n",
      "Epoch: 133 | loss: 0.6923924088478088 \n",
      "Epoch: 134 | loss: 0.6923749446868896 \n",
      "Epoch: 135 | loss: 0.6923571228981018 \n",
      "Epoch: 136 | loss: 0.6923388838768005 \n",
      "Epoch: 137 | loss: 0.6923201084136963 \n",
      "Epoch: 138 | loss: 0.6923009753227234 \n",
      "Epoch: 139 | loss: 0.6922813653945923 \n",
      "Epoch: 140 | loss: 0.6922612190246582 \n",
      "Epoch: 141 | loss: 0.6922405958175659 \n",
      "Epoch: 142 | loss: 0.6922195553779602 \n",
      "Epoch: 143 | loss: 0.6921979784965515 \n",
      "Epoch: 144 | loss: 0.6921757459640503 \n",
      "Epoch: 145 | loss: 0.6921530365943909 \n",
      "Epoch: 146 | loss: 0.6921298503875732 \n",
      "Epoch: 147 | loss: 0.6921059489250183 \n",
      "Epoch: 148 | loss: 0.6920815110206604 \n",
      "Epoch: 149 | loss: 0.6920564770698547 \n",
      "Epoch: 150 | loss: 0.6920307874679565 \n",
      "Epoch: 151 | loss: 0.6920044422149658 \n",
      "Epoch: 152 | loss: 0.6919774413108826 \n",
      "Epoch: 153 | loss: 0.6919498443603516 \n",
      "Epoch: 154 | loss: 0.6919214129447937 \n",
      "Epoch: 155 | loss: 0.6918923854827881 \n",
      "Epoch: 156 | loss: 0.6918624639511108 \n",
      "Epoch: 157 | loss: 0.6918318867683411 \n",
      "Epoch: 158 | loss: 0.6918005347251892 \n",
      "Epoch: 159 | loss: 0.6917683482170105 \n",
      "Epoch: 160 | loss: 0.6917352080345154 \n",
      "Epoch: 161 | loss: 0.691701352596283 \n",
      "Epoch: 162 | loss: 0.6916667222976685 \n",
      "Epoch: 163 | loss: 0.691631019115448 \n",
      "Epoch: 164 | loss: 0.6915944814682007 \n",
      "Epoch: 165 | loss: 0.6915569305419922 \n",
      "Epoch: 166 | loss: 0.6915184855461121 \n",
      "Epoch: 167 | loss: 0.691478967666626 \n",
      "Epoch: 168 | loss: 0.6914384961128235 \n",
      "Epoch: 169 | loss: 0.6913968920707703 \n",
      "Epoch: 170 | loss: 0.6913542151451111 \n",
      "Epoch: 171 | loss: 0.6913104057312012 \n",
      "Epoch: 172 | loss: 0.6912655830383301 \n",
      "Epoch: 173 | loss: 0.6912195682525635 \n",
      "Epoch: 174 | loss: 0.691172182559967 \n",
      "Epoch: 175 | loss: 0.6911236643791199 \n",
      "Epoch: 176 | loss: 0.6910738348960876 \n",
      "Epoch: 177 | loss: 0.6910228729248047 \n",
      "Epoch: 178 | loss: 0.6909704804420471 \n",
      "Epoch: 179 | loss: 0.6909165978431702 \n",
      "Epoch: 180 | loss: 0.6908614635467529 \n",
      "Epoch: 181 | loss: 0.6908047199249268 \n",
      "Epoch: 182 | loss: 0.6907467842102051 \n",
      "Epoch: 183 | loss: 0.6906871199607849 \n",
      "Epoch: 184 | loss: 0.6906259059906006 \n",
      "Epoch: 185 | loss: 0.6905632019042969 \n",
      "Epoch: 186 | loss: 0.6904987096786499 \n",
      "Epoch: 187 | loss: 0.6904326677322388 \n",
      "Epoch: 188 | loss: 0.6903648376464844 \n",
      "Epoch: 189 | loss: 0.6902953386306763 \n",
      "Epoch: 190 | loss: 0.6902239918708801 \n",
      "Epoch: 191 | loss: 0.6901507377624512 \n",
      "Epoch: 192 | loss: 0.690075695514679 \n",
      "Epoch: 193 | loss: 0.6899986267089844 \n",
      "Epoch: 194 | loss: 0.6899195909500122 \n",
      "Epoch: 195 | loss: 0.6898385882377625 \n",
      "Epoch: 196 | loss: 0.689755380153656 \n",
      "Epoch: 197 | loss: 0.6896700859069824 \n",
      "Epoch: 198 | loss: 0.6895826458930969 \n",
      "Epoch: 199 | loss: 0.68949294090271 \n",
      "Epoch: 200 | loss: 0.6894009113311768 \n",
      "Epoch: 201 | loss: 0.6893066167831421 \n",
      "Epoch: 202 | loss: 0.6892098784446716 \n",
      "Epoch: 203 | loss: 0.6891106963157654 \n",
      "Epoch: 204 | loss: 0.6890089511871338 \n",
      "Epoch: 205 | loss: 0.6889047026634216 \n",
      "Epoch: 206 | loss: 0.6887978315353394 \n",
      "Epoch: 207 | loss: 0.6886882185935974 \n",
      "Epoch: 208 | loss: 0.688575804233551 \n",
      "Epoch: 209 | loss: 0.6884607076644897 \n",
      "Epoch: 210 | loss: 0.6883425712585449 \n",
      "Epoch: 211 | loss: 0.6882216930389404 \n",
      "Epoch: 212 | loss: 0.6880977153778076 \n",
      "Epoch: 213 | loss: 0.6879705786705017 \n",
      "Epoch: 214 | loss: 0.6878404021263123 \n",
      "Epoch: 215 | loss: 0.6877070665359497 \n",
      "Epoch: 216 | loss: 0.6875703930854797 \n",
      "Epoch: 217 | loss: 0.6874303221702576 \n",
      "Epoch: 218 | loss: 0.687286913394928 \n",
      "Epoch: 219 | loss: 0.6871399879455566 \n",
      "Epoch: 220 | loss: 0.6869895458221436 \n",
      "Epoch: 221 | loss: 0.6868354678153992 \n",
      "Epoch: 222 | loss: 0.6866776347160339 \n",
      "Epoch: 223 | loss: 0.6865159869194031 \n",
      "Epoch: 224 | loss: 0.6863505840301514 \n",
      "Epoch: 225 | loss: 0.6861810088157654 \n",
      "Epoch: 226 | loss: 0.686007559299469 \n",
      "Epoch: 227 | loss: 0.6858300566673279 \n",
      "Epoch: 228 | loss: 0.6856483221054077 \n",
      "Epoch: 229 | loss: 0.685462236404419 \n",
      "Epoch: 230 | loss: 0.6852719187736511 \n",
      "Epoch: 231 | loss: 0.6850771307945251 \n",
      "Epoch: 232 | loss: 0.6848777532577515 \n",
      "Epoch: 233 | loss: 0.6846737861633301 \n",
      "Epoch: 234 | loss: 0.6844651699066162 \n",
      "Epoch: 235 | loss: 0.684251606464386 \n",
      "Epoch: 236 | loss: 0.6840333342552185 \n",
      "Epoch: 237 | loss: 0.6838100552558899 \n",
      "Epoch: 238 | loss: 0.6835815906524658 \n",
      "Epoch: 239 | loss: 0.6833480596542358 \n",
      "Epoch: 240 | loss: 0.6831092834472656 \n",
      "Epoch: 241 | loss: 0.6828650832176208 \n",
      "Epoch: 242 | loss: 0.6826154589653015 \n",
      "Epoch: 243 | loss: 0.6823602318763733 \n",
      "Epoch: 244 | loss: 0.682099461555481 \n",
      "Epoch: 245 | loss: 0.6818327903747559 \n",
      "Epoch: 246 | loss: 0.6815602779388428 \n",
      "Epoch: 247 | loss: 0.6812818646430969 \n",
      "Epoch: 248 | loss: 0.6809974908828735 \n",
      "Epoch: 249 | loss: 0.680706799030304 \n",
      "Epoch: 250 | loss: 0.680409848690033 \n",
      "Epoch: 251 | loss: 0.680106520652771 \n",
      "Epoch: 252 | loss: 0.679796576499939 \n",
      "Epoch: 253 | loss: 0.6794801950454712 \n",
      "Epoch: 254 | loss: 0.679157018661499 \n",
      "Epoch: 255 | loss: 0.6788270473480225 \n",
      "Epoch: 256 | loss: 0.6784900426864624 \n",
      "Epoch: 257 | loss: 0.6781461238861084 \n",
      "Epoch: 258 | loss: 0.6777949333190918 \n",
      "Epoch: 259 | loss: 0.6774365901947021 \n",
      "Epoch: 260 | loss: 0.677070677280426 \n",
      "Epoch: 261 | loss: 0.676697313785553 \n",
      "Epoch: 262 | loss: 0.6763162016868591 \n",
      "Epoch: 263 | loss: 0.6759275794029236 \n",
      "Epoch: 264 | loss: 0.6755307912826538 \n",
      "Epoch: 265 | loss: 0.6751262545585632 \n",
      "Epoch: 266 | loss: 0.6747134327888489 \n",
      "Epoch: 267 | loss: 0.6742925047874451 \n",
      "Epoch: 268 | loss: 0.6738631725311279 \n",
      "Epoch: 269 | loss: 0.6734253764152527 \n",
      "Epoch: 270 | loss: 0.6729788780212402 \n",
      "Epoch: 271 | loss: 0.6725237965583801 \n",
      "Epoch: 272 | loss: 0.6720597743988037 \n",
      "Epoch: 273 | loss: 0.671586811542511 \n",
      "Epoch: 274 | loss: 0.6711047887802124 \n",
      "Epoch: 275 | loss: 0.6706135272979736 \n",
      "Epoch: 276 | loss: 0.6701129674911499 \n",
      "Epoch: 277 | loss: 0.6696029901504517 \n",
      "Epoch: 278 | loss: 0.6690833568572998 \n",
      "Epoch: 279 | loss: 0.6685541272163391 \n",
      "Epoch: 280 | loss: 0.6680150032043457 \n",
      "Epoch: 281 | loss: 0.6674659252166748 \n",
      "Epoch: 282 | loss: 0.6669068336486816 \n",
      "Epoch: 283 | loss: 0.6663374900817871 \n",
      "Epoch: 284 | loss: 0.665757954120636 \n",
      "Epoch: 285 | loss: 0.6651679873466492 \n",
      "Epoch: 286 | loss: 0.6645675897598267 \n",
      "Epoch: 287 | loss: 0.6639564037322998 \n",
      "Epoch: 288 | loss: 0.6633344888687134 \n",
      "Epoch: 289 | loss: 0.6627017855644226 \n",
      "Epoch: 290 | loss: 0.6620581746101379 \n",
      "Epoch: 291 | loss: 0.6614033579826355 \n",
      "Epoch: 292 | loss: 0.6607374548912048 \n",
      "Epoch: 293 | loss: 0.6600602865219116 \n",
      "Epoch: 294 | loss: 0.6593716144561768 \n",
      "Epoch: 295 | loss: 0.6586715579032898 \n",
      "Epoch: 296 | loss: 0.6579598784446716 \n",
      "Epoch: 297 | loss: 0.657236635684967 \n",
      "Epoch: 298 | loss: 0.6565015316009521 \n",
      "Epoch: 299 | loss: 0.655754566192627 \n",
      "Epoch: 300 | loss: 0.6549957990646362 \n",
      "Epoch: 301 | loss: 0.6542249321937561 \n",
      "Epoch: 302 | loss: 0.6534420251846313 \n",
      "Epoch: 303 | loss: 0.6526468396186829 \n",
      "Epoch: 304 | loss: 0.6518394351005554 \n",
      "Epoch: 305 | loss: 0.6510197520256042 \n",
      "Epoch: 306 | loss: 0.6501876711845398 \n",
      "Epoch: 307 | loss: 0.6493431925773621 \n",
      "Epoch: 308 | loss: 0.6484862565994263 \n",
      "Epoch: 309 | loss: 0.6476167440414429 \n",
      "Epoch: 310 | loss: 0.6467345952987671 \n",
      "Epoch: 311 | loss: 0.6458397507667542 \n",
      "Epoch: 312 | loss: 0.6449323296546936 \n",
      "Epoch: 313 | loss: 0.6440120935440063 \n",
      "Epoch: 314 | loss: 0.6430791616439819 \n",
      "Epoch: 315 | loss: 0.6421334147453308 \n",
      "Epoch: 316 | loss: 0.641174852848053 \n",
      "Epoch: 317 | loss: 0.6402035355567932 \n",
      "Epoch: 318 | loss: 0.639219343662262 \n",
      "Epoch: 319 | loss: 0.6382222175598145 \n",
      "Epoch: 320 | loss: 0.6372122764587402 \n",
      "Epoch: 321 | loss: 0.6361895203590393 \n",
      "Epoch: 322 | loss: 0.6351538896560669 \n",
      "Epoch: 323 | loss: 0.6341054439544678 \n",
      "Epoch: 324 | loss: 0.6330441236495972 \n",
      "Epoch: 325 | loss: 0.6319700479507446 \n",
      "Epoch: 326 | loss: 0.6308830976486206 \n",
      "Epoch: 327 | loss: 0.6297834515571594 \n",
      "Epoch: 328 | loss: 0.6286710500717163 \n",
      "Epoch: 329 | loss: 0.6275460124015808 \n",
      "Epoch: 330 | loss: 0.6264083385467529 \n",
      "Epoch: 331 | loss: 0.6252580285072327 \n",
      "Epoch: 332 | loss: 0.6240953207015991 \n",
      "Epoch: 333 | loss: 0.6229199767112732 \n",
      "Epoch: 334 | loss: 0.6217323541641235 \n",
      "Epoch: 335 | loss: 0.620532214641571 \n",
      "Epoch: 336 | loss: 0.6193199753761292 \n",
      "Epoch: 337 | loss: 0.6180955171585083 \n",
      "Epoch: 338 | loss: 0.6168588995933533 \n",
      "Epoch: 339 | loss: 0.6156103610992432 \n",
      "Epoch: 340 | loss: 0.6143497824668884 \n",
      "Epoch: 341 | loss: 0.6130774021148682 \n",
      "Epoch: 342 | loss: 0.6117934584617615 \n",
      "Epoch: 343 | loss: 0.610497772693634 \n",
      "Epoch: 344 | loss: 0.6091907024383545 \n",
      "Epoch: 345 | loss: 0.6078722476959229 \n",
      "Epoch: 346 | loss: 0.6065424084663391 \n",
      "Epoch: 347 | loss: 0.6052015423774719 \n",
      "Epoch: 348 | loss: 0.6038496494293213 \n",
      "Epoch: 349 | loss: 0.6024869680404663 \n",
      "Epoch: 350 | loss: 0.6011135578155518 \n",
      "Epoch: 351 | loss: 0.5997294783592224 \n",
      "Epoch: 352 | loss: 0.5983350276947021 \n",
      "Epoch: 353 | loss: 0.5969302654266357 \n",
      "Epoch: 354 | loss: 0.5955154895782471 \n",
      "Epoch: 355 | loss: 0.5940907001495361 \n",
      "Epoch: 356 | loss: 0.5926560759544373 \n",
      "Epoch: 357 | loss: 0.5912117958068848 \n",
      "Epoch: 358 | loss: 0.5897580981254578 \n",
      "Epoch: 359 | loss: 0.5882952213287354 \n",
      "Epoch: 360 | loss: 0.5868231058120728 \n",
      "Epoch: 361 | loss: 0.5853420495986938 \n",
      "Epoch: 362 | loss: 0.5838522911071777 \n",
      "Epoch: 363 | loss: 0.582353949546814 \n",
      "Epoch: 364 | loss: 0.5808472633361816 \n",
      "Epoch: 365 | loss: 0.5793323516845703 \n",
      "Epoch: 366 | loss: 0.5778095126152039 \n",
      "Epoch: 367 | loss: 0.5762788653373718 \n",
      "Epoch: 368 | loss: 0.5747405886650085 \n",
      "Epoch: 369 | loss: 0.5731948614120483 \n",
      "Epoch: 370 | loss: 0.5716420412063599 \n",
      "Epoch: 371 | loss: 0.5700823068618774 \n",
      "Epoch: 372 | loss: 0.5685155987739563 \n",
      "Epoch: 373 | loss: 0.5669424533843994 \n",
      "Epoch: 374 | loss: 0.5653629302978516 \n",
      "Epoch: 375 | loss: 0.5637772083282471 \n",
      "Epoch: 376 | loss: 0.5621855854988098 \n",
      "Epoch: 377 | loss: 0.5605881810188293 \n",
      "Epoch: 378 | loss: 0.5589853525161743 \n",
      "Epoch: 379 | loss: 0.5573770999908447 \n",
      "Epoch: 380 | loss: 0.5557639598846436 \n",
      "Epoch: 381 | loss: 0.5541457533836365 \n",
      "Epoch: 382 | loss: 0.5525230169296265 \n",
      "Epoch: 383 | loss: 0.5508958101272583 \n",
      "Epoch: 384 | loss: 0.5492643713951111 \n",
      "Epoch: 385 | loss: 0.5476289391517639 \n",
      "Epoch: 386 | loss: 0.5459897518157959 \n",
      "Epoch: 387 | loss: 0.5443470478057861 \n",
      "Epoch: 388 | loss: 0.5427008867263794 \n",
      "Epoch: 389 | loss: 0.5410516858100891 \n",
      "Epoch: 390 | loss: 0.5393995046615601 \n",
      "Epoch: 391 | loss: 0.5377447009086609 \n",
      "Epoch: 392 | loss: 0.5360873341560364 \n",
      "Epoch: 393 | loss: 0.5344277024269104 \n",
      "Epoch: 394 | loss: 0.5327660441398621 \n",
      "Epoch: 395 | loss: 0.5311025381088257 \n",
      "Epoch: 396 | loss: 0.5294374227523804 \n",
      "Epoch: 397 | loss: 0.5277708172798157 \n",
      "Epoch: 398 | loss: 0.5261030197143555 \n",
      "Epoch: 399 | loss: 0.5244341492652893 \n",
      "Epoch: 400 | loss: 0.5227645635604858 \n",
      "Epoch: 401 | loss: 0.5210942625999451 \n",
      "Epoch: 402 | loss: 0.5194236636161804 \n",
      "Epoch: 403 | loss: 0.5177528262138367 \n",
      "Epoch: 404 | loss: 0.5160819292068481 \n",
      "Epoch: 405 | loss: 0.5144112706184387 \n",
      "Epoch: 406 | loss: 0.512740969657898 \n",
      "Epoch: 407 | loss: 0.5110712051391602 \n",
      "Epoch: 408 | loss: 0.5094022154808044 \n",
      "Epoch: 409 | loss: 0.5077341198921204 \n",
      "Epoch: 410 | loss: 0.506067156791687 \n",
      "Epoch: 411 | loss: 0.5044015645980835 \n",
      "Epoch: 412 | loss: 0.5027374029159546 \n",
      "Epoch: 413 | loss: 0.5010749101638794 \n",
      "Epoch: 414 | loss: 0.49941426515579224 \n",
      "Epoch: 415 | loss: 0.49775564670562744 \n",
      "Epoch: 416 | loss: 0.4960990846157074 \n",
      "Epoch: 417 | loss: 0.49444496631622314 \n",
      "Epoch: 418 | loss: 0.4927932620048523 \n",
      "Epoch: 419 | loss: 0.4911442995071411 \n",
      "Epoch: 420 | loss: 0.4894981384277344 \n",
      "Epoch: 421 | loss: 0.48785486817359924 \n",
      "Epoch: 422 | loss: 0.48621487617492676 \n",
      "Epoch: 423 | loss: 0.484578013420105 \n",
      "Epoch: 424 | loss: 0.4829446077346802 \n",
      "Epoch: 425 | loss: 0.4813147485256195 \n",
      "Epoch: 426 | loss: 0.4796886146068573 \n",
      "Epoch: 427 | loss: 0.47806626558303833 \n",
      "Epoch: 428 | loss: 0.4764479696750641 \n",
      "Epoch: 429 | loss: 0.4748336672782898 \n",
      "Epoch: 430 | loss: 0.47322365641593933 \n",
      "Epoch: 431 | loss: 0.4716179668903351 \n",
      "Epoch: 432 | loss: 0.4700167775154114 \n",
      "Epoch: 433 | loss: 0.4684201180934906 \n",
      "Epoch: 434 | loss: 0.4668281674385071 \n",
      "Epoch: 435 | loss: 0.46524104475975037 \n",
      "Epoch: 436 | loss: 0.4636588990688324 \n",
      "Epoch: 437 | loss: 0.4620817303657532 \n",
      "Epoch: 438 | loss: 0.460509717464447 \n",
      "Epoch: 439 | loss: 0.4589429199695587 \n",
      "Epoch: 440 | loss: 0.4573814868927002 \n",
      "Epoch: 441 | loss: 0.45582541823387146 \n",
      "Epoch: 442 | loss: 0.454274982213974 \n",
      "Epoch: 443 | loss: 0.45273005962371826 \n",
      "Epoch: 444 | loss: 0.45119085907936096 \n",
      "Epoch: 445 | loss: 0.4496574401855469 \n",
      "Epoch: 446 | loss: 0.44812989234924316 \n",
      "Epoch: 447 | loss: 0.44660818576812744 \n",
      "Epoch: 448 | loss: 0.4450925886631012 \n",
      "Epoch: 449 | loss: 0.44358307123184204 \n",
      "Epoch: 450 | loss: 0.44207969307899475 \n",
      "Epoch: 451 | loss: 0.4405825436115265 \n",
      "Epoch: 452 | loss: 0.43909165263175964 \n",
      "Epoch: 453 | loss: 0.4376071095466614 \n",
      "Epoch: 454 | loss: 0.43612903356552124 \n",
      "Epoch: 455 | loss: 0.43465736508369446 \n",
      "Epoch: 456 | loss: 0.4331922233104706 \n",
      "Epoch: 457 | loss: 0.431733638048172 \n",
      "Epoch: 458 | loss: 0.4302816390991211 \n",
      "Epoch: 459 | loss: 0.4288363754749298 \n",
      "Epoch: 460 | loss: 0.427397757768631 \n",
      "Epoch: 461 | loss: 0.42596596479415894 \n",
      "Epoch: 462 | loss: 0.4245408773422241 \n",
      "Epoch: 463 | loss: 0.42312270402908325 \n",
      "Epoch: 464 | loss: 0.4217113256454468 \n",
      "Epoch: 465 | loss: 0.42030686140060425 \n",
      "Epoch: 466 | loss: 0.4189094305038452 \n",
      "Epoch: 467 | loss: 0.41751885414123535 \n",
      "Epoch: 468 | loss: 0.4161352217197418 \n",
      "Epoch: 469 | loss: 0.4147586524486542 \n",
      "Epoch: 470 | loss: 0.4133892059326172 \n",
      "Epoch: 471 | loss: 0.41202667355537415 \n",
      "Epoch: 472 | loss: 0.41067132353782654 \n",
      "Epoch: 473 | loss: 0.4093230366706848 \n",
      "Epoch: 474 | loss: 0.40798187255859375 \n",
      "Epoch: 475 | loss: 0.40664780139923096 \n",
      "Epoch: 476 | loss: 0.4053208827972412 \n",
      "Epoch: 477 | loss: 0.4040010869503021 \n",
      "Epoch: 478 | loss: 0.40268853306770325 \n",
      "Epoch: 479 | loss: 0.40138301253318787 \n",
      "Epoch: 480 | loss: 0.40008479356765747 \n",
      "Epoch: 481 | loss: 0.3987937271595001 \n",
      "Epoch: 482 | loss: 0.39750975370407104 \n",
      "Epoch: 483 | loss: 0.3962330222129822 \n",
      "Epoch: 484 | loss: 0.39496350288391113 \n",
      "Epoch: 485 | loss: 0.39370113611221313 \n",
      "Epoch: 486 | loss: 0.39244595170021057 \n",
      "Epoch: 487 | loss: 0.39119791984558105 \n",
      "Epoch: 488 | loss: 0.3899570405483246 \n",
      "Epoch: 489 | loss: 0.38872337341308594 \n",
      "Epoch: 490 | loss: 0.3874969184398651 \n",
      "Epoch: 491 | loss: 0.3862774670124054 \n",
      "Epoch: 492 | loss: 0.3850652277469635 \n",
      "Epoch: 493 | loss: 0.38386014103889465 \n",
      "Epoch: 494 | loss: 0.3826621174812317 \n",
      "Epoch: 495 | loss: 0.3814712464809418 \n",
      "Epoch: 496 | loss: 0.38028740882873535 \n",
      "Epoch: 497 | loss: 0.3791106343269348 \n",
      "Epoch: 498 | loss: 0.37794098258018494 \n",
      "Epoch: 499 | loss: 0.37677833437919617 \n",
      "Epoch: 500 | loss: 0.3756227195262909 \n",
      "Epoch: 501 | loss: 0.37447410821914673 \n",
      "Epoch: 502 | loss: 0.3733324408531189 \n",
      "Epoch: 503 | loss: 0.37219780683517456 \n",
      "Epoch: 504 | loss: 0.3710700571537018 \n",
      "Epoch: 505 | loss: 0.36994922161102295 \n",
      "Epoch: 506 | loss: 0.36883535981178284 \n",
      "Epoch: 507 | loss: 0.3677283227443695 \n",
      "Epoch: 508 | loss: 0.36662814021110535 \n",
      "Epoch: 509 | loss: 0.36553481221199036 \n",
      "Epoch: 510 | loss: 0.36444827914237976 \n",
      "Epoch: 511 | loss: 0.3633684813976288 \n",
      "Epoch: 512 | loss: 0.3622954487800598 \n",
      "Epoch: 513 | loss: 0.36122918128967285 \n",
      "Epoch: 514 | loss: 0.36016955971717834 \n",
      "Epoch: 515 | loss: 0.35911664366722107 \n",
      "Epoch: 516 | loss: 0.35807037353515625 \n",
      "Epoch: 517 | loss: 0.3570307195186615 \n",
      "Epoch: 518 | loss: 0.35599762201309204 \n",
      "Epoch: 519 | loss: 0.3549710512161255 \n",
      "Epoch: 520 | loss: 0.35395103693008423 \n",
      "Epoch: 521 | loss: 0.3529375195503235 \n",
      "Epoch: 522 | loss: 0.3519304692745209 \n",
      "Epoch: 523 | loss: 0.3509298264980316 \n",
      "Epoch: 524 | loss: 0.3499356210231781 \n",
      "Epoch: 525 | loss: 0.34894776344299316 \n",
      "Epoch: 526 | loss: 0.3479662239551544 \n",
      "Epoch: 527 | loss: 0.34699100255966187 \n",
      "Epoch: 528 | loss: 0.3460220694541931 \n",
      "Epoch: 529 | loss: 0.3450593650341034 \n",
      "Epoch: 530 | loss: 0.3441028892993927 \n",
      "Epoch: 531 | loss: 0.3431524932384491 \n",
      "Epoch: 532 | loss: 0.34220829606056213 \n",
      "Epoch: 533 | loss: 0.34127020835876465 \n",
      "Epoch: 534 | loss: 0.34033825993537903 \n",
      "Epoch: 535 | loss: 0.33941227197647095 \n",
      "Epoch: 536 | loss: 0.3384922742843628 \n",
      "Epoch: 537 | loss: 0.33757826685905457 \n",
      "Epoch: 538 | loss: 0.3366701900959015 \n",
      "Epoch: 539 | loss: 0.33576804399490356 \n",
      "Epoch: 540 | loss: 0.33487170934677124 \n",
      "Epoch: 541 | loss: 0.3339812755584717 \n",
      "Epoch: 542 | loss: 0.33309662342071533 \n",
      "Epoch: 543 | loss: 0.33221766352653503 \n",
      "Epoch: 544 | loss: 0.33134451508522034 \n",
      "Epoch: 545 | loss: 0.3304770290851593 \n",
      "Epoch: 546 | loss: 0.32961517572402954 \n",
      "Epoch: 547 | loss: 0.32875895500183105 \n",
      "Epoch: 548 | loss: 0.32790839672088623 \n",
      "Epoch: 549 | loss: 0.32706332206726074 \n",
      "Epoch: 550 | loss: 0.326223760843277 \n",
      "Epoch: 551 | loss: 0.32538971304893494 \n",
      "Epoch: 552 | loss: 0.32456108927726746 \n",
      "Epoch: 553 | loss: 0.32373785972595215 \n",
      "Epoch: 554 | loss: 0.322920024394989 \n",
      "Epoch: 555 | loss: 0.32210761308670044 \n",
      "Epoch: 556 | loss: 0.3213004171848297 \n",
      "Epoch: 557 | loss: 0.3204984962940216 \n",
      "Epoch: 558 | loss: 0.3197018802165985 \n",
      "Epoch: 559 | loss: 0.3189104199409485 \n",
      "Epoch: 560 | loss: 0.31812411546707153 \n",
      "Epoch: 561 | loss: 0.31734296679496765 \n",
      "Epoch: 562 | loss: 0.31656691431999207 \n",
      "Epoch: 563 | loss: 0.3157959580421448 \n",
      "Epoch: 564 | loss: 0.3150300085544586 \n",
      "Epoch: 565 | loss: 0.3142690360546112 \n",
      "Epoch: 566 | loss: 0.3135130703449249 \n",
      "Epoch: 567 | loss: 0.3127620220184326 \n",
      "Epoch: 568 | loss: 0.3120158910751343 \n",
      "Epoch: 569 | loss: 0.31127458810806274 \n",
      "Epoch: 570 | loss: 0.3105381429195404 \n",
      "Epoch: 571 | loss: 0.3098064661026001 \n",
      "Epoch: 572 | loss: 0.3090796172618866 \n",
      "Epoch: 573 | loss: 0.3083574175834656 \n",
      "Epoch: 574 | loss: 0.3076399564743042 \n",
      "Epoch: 575 | loss: 0.3069271743297577 \n",
      "Epoch: 576 | loss: 0.3062189817428589 \n",
      "Epoch: 577 | loss: 0.30551543831825256 \n",
      "Epoch: 578 | loss: 0.30481642484664917 \n",
      "Epoch: 579 | loss: 0.3041219711303711 \n",
      "Epoch: 580 | loss: 0.3034319579601288 \n",
      "Epoch: 581 | loss: 0.3027464747428894 \n",
      "Epoch: 582 | loss: 0.3020654320716858 \n",
      "Epoch: 583 | loss: 0.30138877034187317 \n",
      "Epoch: 584 | loss: 0.30071648955345154 \n",
      "Epoch: 585 | loss: 0.3000485599040985 \n",
      "Epoch: 586 | loss: 0.2993849515914917 \n",
      "Epoch: 587 | loss: 0.2987256348133087 \n",
      "Epoch: 588 | loss: 0.2980705201625824 \n",
      "Epoch: 589 | loss: 0.29741963744163513 \n",
      "Epoch: 590 | loss: 0.29677295684814453 \n",
      "Epoch: 591 | loss: 0.2961304485797882 \n",
      "Epoch: 592 | loss: 0.2954920530319214 \n",
      "Epoch: 593 | loss: 0.29485777020454407 \n",
      "Epoch: 594 | loss: 0.2942275404930115 \n",
      "Epoch: 595 | loss: 0.2936013638973236 \n",
      "Epoch: 596 | loss: 0.2929791510105133 \n",
      "Epoch: 597 | loss: 0.29236096143722534 \n",
      "Epoch: 598 | loss: 0.29174676537513733 \n",
      "Epoch: 599 | loss: 0.2911364436149597 \n",
      "Epoch: 600 | loss: 0.2905300557613373 \n",
      "Epoch: 601 | loss: 0.28992748260498047 \n",
      "Epoch: 602 | loss: 0.28932875394821167 \n",
      "Epoch: 603 | loss: 0.2887338399887085 \n",
      "Epoch: 604 | loss: 0.28814274072647095 \n",
      "Epoch: 605 | loss: 0.2875553369522095 \n",
      "Epoch: 606 | loss: 0.28697168827056885 \n",
      "Epoch: 607 | loss: 0.2863917648792267 \n",
      "Epoch: 608 | loss: 0.2858154773712158 \n",
      "Epoch: 609 | loss: 0.28524285554885864 \n",
      "Epoch: 610 | loss: 0.284673810005188 \n",
      "Epoch: 611 | loss: 0.28410840034484863 \n",
      "Epoch: 612 | loss: 0.2835465371608734 \n",
      "Epoch: 613 | loss: 0.28298819065093994 \n",
      "Epoch: 614 | loss: 0.28243333101272583 \n",
      "Epoch: 615 | loss: 0.28188201785087585 \n",
      "Epoch: 616 | loss: 0.28133416175842285 \n",
      "Epoch: 617 | loss: 0.28078973293304443 \n",
      "Epoch: 618 | loss: 0.2802486717700958 \n",
      "Epoch: 619 | loss: 0.2797110676765442 \n",
      "Epoch: 620 | loss: 0.2791767418384552 \n",
      "Epoch: 621 | loss: 0.27864575386047363 \n",
      "Epoch: 622 | loss: 0.2781181037425995 \n",
      "Epoch: 623 | loss: 0.277593731880188 \n",
      "Epoch: 624 | loss: 0.27707263827323914 \n",
      "Epoch: 625 | loss: 0.27655476331710815 \n",
      "Epoch: 626 | loss: 0.27604007720947266 \n",
      "Epoch: 627 | loss: 0.27552860975265503 \n",
      "Epoch: 628 | loss: 0.2750203013420105 \n",
      "Epoch: 629 | loss: 0.27451515197753906 \n",
      "Epoch: 630 | loss: 0.27401307225227356 \n",
      "Epoch: 631 | loss: 0.2735140919685364 \n",
      "Epoch: 632 | loss: 0.2730181813240051 \n",
      "Epoch: 633 | loss: 0.2725253701210022 \n",
      "Epoch: 634 | loss: 0.27203550934791565 \n",
      "Epoch: 635 | loss: 0.27154868841171265 \n",
      "Epoch: 636 | loss: 0.271064817905426 \n",
      "Epoch: 637 | loss: 0.27058398723602295 \n",
      "Epoch: 638 | loss: 0.2701059877872467 \n",
      "Epoch: 639 | loss: 0.2696309983730316 \n",
      "Epoch: 640 | loss: 0.26915881037712097 \n",
      "Epoch: 641 | loss: 0.2686895430088043 \n",
      "Epoch: 642 | loss: 0.2682231068611145 \n",
      "Epoch: 643 | loss: 0.2677595317363739 \n",
      "Epoch: 644 | loss: 0.26729872822761536 \n",
      "Epoch: 645 | loss: 0.26684072613716125 \n",
      "Epoch: 646 | loss: 0.2663854658603668 \n",
      "Epoch: 647 | loss: 0.26593297719955444 \n",
      "Epoch: 648 | loss: 0.2654832601547241 \n",
      "Epoch: 649 | loss: 0.2650361657142639 \n",
      "Epoch: 650 | loss: 0.2645917534828186 \n",
      "Epoch: 651 | loss: 0.26415008306503296 \n",
      "Epoch: 652 | loss: 0.26371100544929504 \n",
      "Epoch: 653 | loss: 0.26327455043792725 \n",
      "Epoch: 654 | loss: 0.26284071803092957 \n",
      "Epoch: 655 | loss: 0.2624094784259796 \n",
      "Epoch: 656 | loss: 0.2619807720184326 \n",
      "Epoch: 657 | loss: 0.2615545988082886 \n",
      "Epoch: 658 | loss: 0.26113101840019226 \n",
      "Epoch: 659 | loss: 0.2607099115848541 \n",
      "Epoch: 660 | loss: 0.26029133796691895 \n",
      "Epoch: 661 | loss: 0.25987520813941956 \n",
      "Epoch: 662 | loss: 0.25946149230003357 \n",
      "Epoch: 663 | loss: 0.25905027985572815 \n",
      "Epoch: 664 | loss: 0.25864148139953613 \n",
      "Epoch: 665 | loss: 0.25823506712913513 \n",
      "Epoch: 666 | loss: 0.25783103704452515 \n",
      "Epoch: 667 | loss: 0.2574293911457062 \n",
      "Epoch: 668 | loss: 0.25703004002571106 \n",
      "Epoch: 669 | loss: 0.25663310289382935 \n",
      "Epoch: 670 | loss: 0.2562384009361267 \n",
      "Epoch: 671 | loss: 0.2558460533618927 \n",
      "Epoch: 672 | loss: 0.25545597076416016 \n",
      "Epoch: 673 | loss: 0.25506818294525146 \n",
      "Epoch: 674 | loss: 0.25468260049819946 \n",
      "Epoch: 675 | loss: 0.2542992830276489 \n",
      "Epoch: 676 | loss: 0.2539181411266327 \n",
      "Epoch: 677 | loss: 0.25353920459747314 \n",
      "Epoch: 678 | loss: 0.2531625032424927 \n",
      "Epoch: 679 | loss: 0.25278791785240173 \n",
      "Epoch: 680 | loss: 0.2524155080318451 \n",
      "Epoch: 681 | loss: 0.252045214176178 \n",
      "Epoch: 682 | loss: 0.2516770362854004 \n",
      "Epoch: 683 | loss: 0.2513110041618347 \n",
      "Epoch: 684 | loss: 0.2509470283985138 \n",
      "Epoch: 685 | loss: 0.2505851686000824 \n",
      "Epoch: 686 | loss: 0.250225305557251 \n",
      "Epoch: 687 | loss: 0.24986755847930908 \n",
      "Epoch: 688 | loss: 0.24951177835464478 \n",
      "Epoch: 689 | loss: 0.24915803968906403 \n",
      "Epoch: 690 | loss: 0.24880628287792206 \n",
      "Epoch: 691 | loss: 0.24845656752586365 \n",
      "Epoch: 692 | loss: 0.24810877442359924 \n",
      "Epoch: 693 | loss: 0.24776296317577362 \n",
      "Epoch: 694 | loss: 0.247419074177742 \n",
      "Epoch: 695 | loss: 0.2470771223306656 \n",
      "Epoch: 696 | loss: 0.24673709273338318 \n",
      "Epoch: 697 | loss: 0.24639900028705597 \n",
      "Epoch: 698 | loss: 0.2460627406835556 \n",
      "Epoch: 699 | loss: 0.24572837352752686 \n",
      "Epoch: 700 | loss: 0.24539588391780853 \n",
      "Epoch: 701 | loss: 0.24506522715091705 \n",
      "Epoch: 702 | loss: 0.2447364181280136 \n",
      "Epoch: 703 | loss: 0.24440941214561462 \n",
      "Epoch: 704 | loss: 0.24408423900604248 \n",
      "Epoch: 705 | loss: 0.2437608540058136 \n",
      "Epoch: 706 | loss: 0.24343925714492798 \n",
      "Epoch: 707 | loss: 0.24311944842338562 \n",
      "Epoch: 708 | loss: 0.24280133843421936 \n",
      "Epoch: 709 | loss: 0.24248503148555756 \n",
      "Epoch: 710 | loss: 0.24217040836811066 \n",
      "Epoch: 711 | loss: 0.24185757339000702 \n",
      "Epoch: 712 | loss: 0.2415464073419571 \n",
      "Epoch: 713 | loss: 0.24123694002628326 \n",
      "Epoch: 714 | loss: 0.24092918634414673 \n",
      "Epoch: 715 | loss: 0.24062304198741913 \n",
      "Epoch: 716 | loss: 0.24031859636306763 \n",
      "Epoch: 717 | loss: 0.24001583456993103 \n",
      "Epoch: 718 | loss: 0.2397146373987198 \n",
      "Epoch: 719 | loss: 0.23941509425640106 \n",
      "Epoch: 720 | loss: 0.23911719024181366 \n",
      "Epoch: 721 | loss: 0.2388208955526352 \n",
      "Epoch: 722 | loss: 0.2385261505842209 \n",
      "Epoch: 723 | loss: 0.2382330298423767 \n",
      "Epoch: 724 | loss: 0.2379414588212967 \n",
      "Epoch: 725 | loss: 0.23765146732330322 \n",
      "Epoch: 726 | loss: 0.23736298084259033 \n",
      "Epoch: 727 | loss: 0.237076073884964 \n",
      "Epoch: 728 | loss: 0.2367907017469406 \n",
      "Epoch: 729 | loss: 0.23650680482387543 \n",
      "Epoch: 730 | loss: 0.2362244427204132 \n",
      "Epoch: 731 | loss: 0.23594354093074799 \n",
      "Epoch: 732 | loss: 0.23566414415836334 \n",
      "Epoch: 733 | loss: 0.23538623750209808 \n",
      "Epoch: 734 | loss: 0.23510979115962982 \n",
      "Epoch: 735 | loss: 0.23483479022979736 \n",
      "Epoch: 736 | loss: 0.2345612347126007 \n",
      "Epoch: 737 | loss: 0.23428910970687866 \n",
      "Epoch: 738 | loss: 0.23401841521263123 \n",
      "Epoch: 739 | loss: 0.2337491661310196 \n",
      "Epoch: 740 | loss: 0.2334812581539154 \n",
      "Epoch: 741 | loss: 0.23321479558944702 \n",
      "Epoch: 742 | loss: 0.23294971883296967 \n",
      "Epoch: 743 | loss: 0.23268596827983856 \n",
      "Epoch: 744 | loss: 0.23242363333702087 \n",
      "Epoch: 745 | loss: 0.23216266930103302 \n",
      "Epoch: 746 | loss: 0.23190301656723022 \n",
      "Epoch: 747 | loss: 0.23164471983909607 \n",
      "Epoch: 748 | loss: 0.23138773441314697 \n",
      "Epoch: 749 | loss: 0.23113209009170532 \n",
      "Epoch: 750 | loss: 0.23087774217128754 \n",
      "Epoch: 751 | loss: 0.230624720454216 \n",
      "Epoch: 752 | loss: 0.23037295043468475 \n",
      "Epoch: 753 | loss: 0.23012250661849976 \n",
      "Epoch: 754 | loss: 0.22987331449985504 \n",
      "Epoch: 755 | loss: 0.229625403881073 \n",
      "Epoch: 756 | loss: 0.22937874495983124 \n",
      "Epoch: 757 | loss: 0.22913335263729095 \n",
      "Epoch: 758 | loss: 0.22888919711112976 \n",
      "Epoch: 759 | loss: 0.22864626348018646 \n",
      "Epoch: 760 | loss: 0.22840456664562225 \n",
      "Epoch: 761 | loss: 0.22816410660743713 \n",
      "Epoch: 762 | loss: 0.22792480885982513 \n",
      "Epoch: 763 | loss: 0.22768676280975342 \n",
      "Epoch: 764 | loss: 0.22744989395141602 \n",
      "Epoch: 765 | loss: 0.22721420228481293 \n",
      "Epoch: 766 | loss: 0.22697967290878296 \n",
      "Epoch: 767 | loss: 0.2267463505268097 \n",
      "Epoch: 768 | loss: 0.22651416063308716 \n",
      "Epoch: 769 | loss: 0.22628311812877655 \n",
      "Epoch: 770 | loss: 0.22605325281620026 \n",
      "Epoch: 771 | loss: 0.2258244901895523 \n",
      "Epoch: 772 | loss: 0.22559688985347748 \n",
      "Epoch: 773 | loss: 0.22537042200565338 \n",
      "Epoch: 774 | loss: 0.22514507174491882 \n",
      "Epoch: 775 | loss: 0.22492080926895142 \n",
      "Epoch: 776 | loss: 0.22469763457775116 \n",
      "Epoch: 777 | loss: 0.22447557747364044 \n",
      "Epoch: 778 | loss: 0.22425459325313568 \n",
      "Epoch: 779 | loss: 0.22403471171855927 \n",
      "Epoch: 780 | loss: 0.2238159030675888 \n",
      "Epoch: 781 | loss: 0.22359812259674072 \n",
      "Epoch: 782 | loss: 0.2233814001083374 \n",
      "Epoch: 783 | loss: 0.22316576540470123 \n",
      "Epoch: 784 | loss: 0.22295114398002625 \n",
      "Epoch: 785 | loss: 0.22273758053779602 \n",
      "Epoch: 786 | loss: 0.22252506017684937 \n",
      "Epoch: 787 | loss: 0.22231358289718628 \n",
      "Epoch: 788 | loss: 0.2221030592918396 \n",
      "Epoch: 789 | loss: 0.22189359366893768 \n",
      "Epoch: 790 | loss: 0.22168512642383575 \n",
      "Epoch: 791 | loss: 0.22147762775421143 \n",
      "Epoch: 792 | loss: 0.22127115726470947 \n",
      "Epoch: 793 | loss: 0.2210656702518463 \n",
      "Epoch: 794 | loss: 0.22086115181446075 \n",
      "Epoch: 795 | loss: 0.2206576019525528 \n",
      "Epoch: 796 | loss: 0.22045502066612244 \n",
      "Epoch: 797 | loss: 0.22025339305400848 \n",
      "Epoch: 798 | loss: 0.22005271911621094 \n",
      "Epoch: 799 | loss: 0.219853013753891 \n",
      "Epoch: 800 | loss: 0.21965426206588745 \n",
      "Epoch: 801 | loss: 0.21945640444755554 \n",
      "Epoch: 802 | loss: 0.21925948560237885 \n",
      "Epoch: 803 | loss: 0.21906350553035736 \n",
      "Epoch: 804 | loss: 0.2188684493303299 \n",
      "Epoch: 805 | loss: 0.21867427229881287 \n",
      "Epoch: 806 | loss: 0.21848104894161224 \n",
      "Epoch: 807 | loss: 0.21828870475292206 \n",
      "Epoch: 808 | loss: 0.2180972546339035 \n",
      "Epoch: 809 | loss: 0.2179066687822342 \n",
      "Epoch: 810 | loss: 0.2177169770002365 \n",
      "Epoch: 811 | loss: 0.21752819418907166 \n",
      "Epoch: 812 | loss: 0.21734026074409485 \n",
      "Epoch: 813 | loss: 0.21715320646762848 \n",
      "Epoch: 814 | loss: 0.21696697175502777 \n",
      "Epoch: 815 | loss: 0.2167816311120987 \n",
      "Epoch: 816 | loss: 0.21659715473651886 \n",
      "Epoch: 817 | loss: 0.2164134979248047 \n",
      "Epoch: 818 | loss: 0.21623067557811737 \n",
      "Epoch: 819 | loss: 0.21604867279529572 \n",
      "Epoch: 820 | loss: 0.2158675193786621 \n",
      "Epoch: 821 | loss: 0.21568723022937775 \n",
      "Epoch: 822 | loss: 0.21550770103931427 \n",
      "Epoch: 823 | loss: 0.21532903611660004 \n",
      "Epoch: 824 | loss: 0.21515116095542908 \n",
      "Epoch: 825 | loss: 0.21497410535812378 \n",
      "Epoch: 826 | loss: 0.21479779481887817 \n",
      "Epoch: 827 | loss: 0.21462230384349823 \n",
      "Epoch: 828 | loss: 0.21444761753082275 \n",
      "Epoch: 829 | loss: 0.21427367627620697 \n",
      "Epoch: 830 | loss: 0.21410056948661804 \n",
      "Epoch: 831 | loss: 0.2139282077550888 \n",
      "Epoch: 832 | loss: 0.21375662088394165 \n",
      "Epoch: 833 | loss: 0.2135857790708542 \n",
      "Epoch: 834 | loss: 0.2134157121181488 \n",
      "Epoch: 835 | loss: 0.2132464051246643 \n",
      "Epoch: 836 | loss: 0.2130778282880783 \n",
      "Epoch: 837 | loss: 0.2129100263118744 \n",
      "Epoch: 838 | loss: 0.21274298429489136 \n",
      "Epoch: 839 | loss: 0.21257659792900085 \n",
      "Epoch: 840 | loss: 0.21241101622581482 \n",
      "Epoch: 841 | loss: 0.2122461497783661 \n",
      "Epoch: 842 | loss: 0.21208199858665466 \n",
      "Epoch: 843 | loss: 0.21191857755184174 \n",
      "Epoch: 844 | loss: 0.2117559015750885 \n",
      "Epoch: 845 | loss: 0.2115938514471054 \n",
      "Epoch: 846 | loss: 0.2114325761795044 \n",
      "Epoch: 847 | loss: 0.2112719714641571 \n",
      "Epoch: 848 | loss: 0.21111206710338593 \n",
      "Epoch: 849 | loss: 0.21095287799835205 \n",
      "Epoch: 850 | loss: 0.2107943296432495 \n",
      "Epoch: 851 | loss: 0.21063652634620667 \n",
      "Epoch: 852 | loss: 0.21047937870025635 \n",
      "Epoch: 853 | loss: 0.21032287180423737 \n",
      "Epoch: 854 | loss: 0.2101670801639557 \n",
      "Epoch: 855 | loss: 0.21001195907592773 \n",
      "Epoch: 856 | loss: 0.20985746383666992 \n",
      "Epoch: 857 | loss: 0.20970366895198822 \n",
      "Epoch: 858 | loss: 0.20955052971839905 \n",
      "Epoch: 859 | loss: 0.2093980610370636 \n",
      "Epoch: 860 | loss: 0.2092461735010147 \n",
      "Epoch: 861 | loss: 0.20909501612186432 \n",
      "Epoch: 862 | loss: 0.20894445478916168 \n",
      "Epoch: 863 | loss: 0.2087945193052292 \n",
      "Epoch: 864 | loss: 0.20864525437355042 \n",
      "Epoch: 865 | loss: 0.2084965705871582 \n",
      "Epoch: 866 | loss: 0.2083485722541809 \n",
      "Epoch: 867 | loss: 0.20820112526416779 \n",
      "Epoch: 868 | loss: 0.2080543488264084 \n",
      "Epoch: 869 | loss: 0.20790819823741913 \n",
      "Epoch: 870 | loss: 0.20776262879371643 \n",
      "Epoch: 871 | loss: 0.2076176404953003 \n",
      "Epoch: 872 | loss: 0.20747333765029907 \n",
      "Epoch: 873 | loss: 0.20732955634593964 \n",
      "Epoch: 874 | loss: 0.20718641579151154 \n",
      "Epoch: 875 | loss: 0.20704385638237 \n",
      "Epoch: 876 | loss: 0.20690187811851501 \n",
      "Epoch: 877 | loss: 0.20676052570343018 \n",
      "Epoch: 878 | loss: 0.2066197246313095 \n",
      "Epoch: 879 | loss: 0.2064795047044754 \n",
      "Epoch: 880 | loss: 0.20633989572525024 \n",
      "Epoch: 881 | loss: 0.20620082318782806 \n",
      "Epoch: 882 | loss: 0.20606236159801483 \n",
      "Epoch: 883 | loss: 0.20592442154884338 \n",
      "Epoch: 884 | loss: 0.2057870477437973 \n",
      "Epoch: 885 | loss: 0.20565025508403778 \n",
      "Epoch: 886 | loss: 0.20551402866840363 \n",
      "Epoch: 887 | loss: 0.20537832379341125 \n",
      "Epoch: 888 | loss: 0.20524317026138306 \n",
      "Epoch: 889 | loss: 0.20510858297348022 \n",
      "Epoch: 890 | loss: 0.20497457683086395 \n",
      "Epoch: 891 | loss: 0.20484107732772827 \n",
      "Epoch: 892 | loss: 0.20470808446407318 \n",
      "Epoch: 893 | loss: 0.20457571744918823 \n",
      "Epoch: 894 | loss: 0.2044437974691391 \n",
      "Epoch: 895 | loss: 0.20431244373321533 \n",
      "Epoch: 896 | loss: 0.20418161153793335 \n",
      "Epoch: 897 | loss: 0.20405128598213196 \n",
      "Epoch: 898 | loss: 0.20392151176929474 \n",
      "Epoch: 899 | loss: 0.20379222929477692 \n",
      "Epoch: 900 | loss: 0.20366348326206207 \n",
      "Epoch: 901 | loss: 0.20353522896766663 \n",
      "Epoch: 902 | loss: 0.20340751111507416 \n",
      "Epoch: 903 | loss: 0.2032802700996399 \n",
      "Epoch: 904 | loss: 0.20315353572368622 \n",
      "Epoch: 905 | loss: 0.20302732288837433 \n",
      "Epoch: 906 | loss: 0.20290161669254303 \n",
      "Epoch: 907 | loss: 0.20277638733386993 \n",
      "Epoch: 908 | loss: 0.20265167951583862 \n",
      "Epoch: 909 | loss: 0.20252740383148193 \n",
      "Epoch: 910 | loss: 0.20240363478660583 \n",
      "Epoch: 911 | loss: 0.20228037238121033 \n",
      "Epoch: 912 | loss: 0.20215757191181183 \n",
      "Epoch: 913 | loss: 0.20203527808189392 \n",
      "Epoch: 914 | loss: 0.2019134759902954 \n",
      "Epoch: 915 | loss: 0.20179206132888794 \n",
      "Epoch: 916 | loss: 0.20167119801044464 \n",
      "Epoch: 917 | loss: 0.20155076682567596 \n",
      "Epoch: 918 | loss: 0.2014308124780655 \n",
      "Epoch: 919 | loss: 0.20131132006645203 \n",
      "Epoch: 920 | loss: 0.20119228959083557 \n",
      "Epoch: 921 | loss: 0.20107369124889374 \n",
      "Epoch: 922 | loss: 0.2009555846452713 \n",
      "Epoch: 923 | loss: 0.2008379101753235 \n",
      "Epoch: 924 | loss: 0.20072072744369507 \n",
      "Epoch: 925 | loss: 0.20060397684574127 \n",
      "Epoch: 926 | loss: 0.2004876285791397 \n",
      "Epoch: 927 | loss: 0.20037178695201874 \n",
      "Epoch: 928 | loss: 0.20025634765625 \n",
      "Epoch: 929 | loss: 0.20014135539531708 \n",
      "Epoch: 930 | loss: 0.20002681016921997 \n",
      "Epoch: 931 | loss: 0.1999126821756363 \n",
      "Epoch: 932 | loss: 0.19979900121688843 \n",
      "Epoch: 933 | loss: 0.19968575239181519 \n",
      "Epoch: 934 | loss: 0.19957290589809418 \n",
      "Epoch: 935 | loss: 0.19946050643920898 \n",
      "Epoch: 936 | loss: 0.19934852421283722 \n",
      "Epoch: 937 | loss: 0.19923698902130127 \n",
      "Epoch: 938 | loss: 0.19912584125995636 \n",
      "Epoch: 939 | loss: 0.19901509582996368 \n",
      "Epoch: 940 | loss: 0.19890479743480682 \n",
      "Epoch: 941 | loss: 0.1987949013710022 \n",
      "Epoch: 942 | loss: 0.198685422539711 \n",
      "Epoch: 943 | loss: 0.19857633113861084 \n",
      "Epoch: 944 | loss: 0.1984676718711853 \n",
      "Epoch: 945 | loss: 0.1983594000339508 \n",
      "Epoch: 946 | loss: 0.19825154542922974 \n",
      "Epoch: 947 | loss: 0.1981440782546997 \n",
      "Epoch: 948 | loss: 0.1980370283126831 \n",
      "Epoch: 949 | loss: 0.19793035089969635 \n",
      "Epoch: 950 | loss: 0.19782407581806183 \n",
      "Epoch: 951 | loss: 0.19771815836429596 \n",
      "Epoch: 952 | loss: 0.1976126879453659 \n",
      "Epoch: 953 | loss: 0.1975075602531433 \n",
      "Epoch: 954 | loss: 0.19740286469459534 \n",
      "Epoch: 955 | loss: 0.19729851186275482 \n",
      "Epoch: 956 | loss: 0.19719450175762177 \n",
      "Epoch: 957 | loss: 0.1970909684896469 \n",
      "Epoch: 958 | loss: 0.19698774814605713 \n",
      "Epoch: 959 | loss: 0.19688493013381958 \n",
      "Epoch: 960 | loss: 0.19678251445293427 \n",
      "Epoch: 961 | loss: 0.19668039679527283 \n",
      "Epoch: 962 | loss: 0.19657869637012482 \n",
      "Epoch: 963 | loss: 0.19647735357284546 \n",
      "Epoch: 964 | loss: 0.19637638330459595 \n",
      "Epoch: 965 | loss: 0.19627578556537628 \n",
      "Epoch: 966 | loss: 0.19617553055286407 \n",
      "Epoch: 967 | loss: 0.1960756480693817 \n",
      "Epoch: 968 | loss: 0.1959761083126068 \n",
      "Epoch: 969 | loss: 0.19587694108486176 \n",
      "Epoch: 970 | loss: 0.19577814638614655 \n",
      "Epoch: 971 | loss: 0.1956796646118164 \n",
      "Epoch: 972 | loss: 0.1955815702676773 \n",
      "Epoch: 973 | loss: 0.19548380374908447 \n",
      "Epoch: 974 | loss: 0.1953863948583603 \n",
      "Epoch: 975 | loss: 0.19528934359550476 \n",
      "Epoch: 976 | loss: 0.1951926201581955 \n",
      "Epoch: 977 | loss: 0.1950962394475937 \n",
      "Epoch: 978 | loss: 0.19500021636486053 \n",
      "Epoch: 979 | loss: 0.19490452110767365 \n",
      "Epoch: 980 | loss: 0.1948091834783554 \n",
      "Epoch: 981 | loss: 0.19471415877342224 \n",
      "Epoch: 982 | loss: 0.19461950659751892 \n",
      "Epoch: 983 | loss: 0.19452516734600067 \n",
      "Epoch: 984 | loss: 0.1944311112165451 \n",
      "Epoch: 985 | loss: 0.19433744251728058 \n",
      "Epoch: 986 | loss: 0.19424410164356232 \n",
      "Epoch: 987 | loss: 0.19415107369422913 \n",
      "Epoch: 988 | loss: 0.1940583735704422 \n",
      "Epoch: 989 | loss: 0.19396598637104034 \n",
      "Epoch: 990 | loss: 0.19387394189834595 \n",
      "Epoch: 991 | loss: 0.19378218054771423 \n",
      "Epoch: 992 | loss: 0.19369079172611237 \n",
      "Epoch: 993 | loss: 0.19359968602657318 \n",
      "Epoch: 994 | loss: 0.19350890815258026 \n",
      "Epoch: 995 | loss: 0.1934184581041336 \n",
      "Epoch: 996 | loss: 0.19332830607891083 \n",
      "Epoch: 997 | loss: 0.19323846697807312 \n",
      "Epoch: 998 | loss: 0.19314894080162048 \n",
      "Epoch: 999 | loss: 0.19305971264839172 \n",
      "Epoch: 1000 | loss: 0.19297081232070923 \n"
     ]
    }
   ],
   "source": [
    "# train a neural network (feel free to choose the num_hidden1 and num_hidden2)\n",
    "# on the dataset in data.csv file\n",
    "# You'll have fun with conflicting shapes and types and tensors, but\n",
    "# you'll get those errors anyway. Let's go into the wild and learn\n",
    "# by reading the errors and trying to understand them! :)\n",
    "# You can always use the provided Workbook\n",
    "\n",
    "X = T.from_numpy(df[[0,1]].values).float()\n",
    "y = T.from_numpy(df[[2]].values).float()\n",
    "\n",
    "\n",
    "x_test, y_test, x_train, y_train = train_test_split(X, y, train_size=0.7, random_state=73)\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() #reset gradients\n",
    "\n",
    "    pred = net.forward(X) #make the prediction\n",
    "\n",
    "    loss = criterion(pred, y) #computing the loss\n",
    "\n",
    "    loss.backward() #passing backwars\n",
    "\n",
    "    optimizer.step() #saving weights\n",
    "\n",
    "    print(f'Epoch: {epoch + 1} | loss: {loss.item()} ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3606f1f72cab31e12ded3fd4dc568aeec6faa77d43eaca4ad210e84657d2ac3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('strive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
